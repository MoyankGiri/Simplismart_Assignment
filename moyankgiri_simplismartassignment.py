# -*- coding: utf-8 -*-
"""MoyankGiri_SimplismartAssignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ZfFbB379aN6DEvM7OKC_SFzajFg6VHj
"""

!pip install -U bitsandbytes transformers

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import infer_auto_device_map, init_empty_weights
from time import time
import warnings


# Suppress all warnings
warnings.filterwarnings("ignore")

# Define constants for benchmarking
CONCURRENCY = 32
INPUT_TOKENS = 128
OUTPUT_TOKENS = 128
TARGET_THROUGHPUT = 200  # tokens/sec

def load_and_optimize_model(model_name):
    """Load and optimize the model for inference."""
    print("Loading and optimizing the model...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Ensure the tokenizer has a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token or tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # Load the model with auto device map for efficient layer placement
    with init_empty_weights():
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
        device_map = infer_auto_device_map(model, max_memory={0: "16GiB"})

    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16, device_map=device_map
    )

    model.eval()  # Set model to evaluation mode
    return model, tokenizer

def benchmark_model(model, tokenizer):
    """Benchmark the model for throughput."""
    print("Running benchmarks...")
    # Generate dummy prompts for concurrency testing
    dummy_prompt = "This is a test prompt."[:INPUT_TOKENS]
    inputs = [dummy_prompt for _ in range(CONCURRENCY)]

    # Tokenize inputs
    tokenized_inputs = tokenizer(inputs, return_tensors="pt", padding=True, truncation=True).to("cuda")

    # Measure time for inference
    start_time = time()
    with torch.no_grad():
        outputs = model.generate(
            input_ids=tokenized_inputs["input_ids"],
            attention_mask=tokenized_inputs["attention_mask"],
            max_new_tokens=OUTPUT_TOKENS
        )
    end_time = time()

    # Calculate throughput
    total_tokens = (INPUT_TOKENS + OUTPUT_TOKENS) * CONCURRENCY
    throughput = total_tokens / (end_time - start_time)

    print(f"Benchmark Results:")
    print(f"  Total Throughput: {throughput:.2f} tokens/sec")
    print(f"  Target Throughput: {TARGET_THROUGHPUT} tokens/sec")

    if throughput >= TARGET_THROUGHPUT:
        print("Benchmark passed! The model meets the throughput requirement.")
    else:
        print("Benchmark failed. Consider further optimizations.")


def run_inference(model, tokenizer):
    """Run inference interactively."""
    print("\nReady for inference. Type 'exit' to quit.")
    while True:
        prompt = input("Enter your prompt: ")
        if prompt.lower() == "exit":
            break

        # Tokenize input
        tokenized_input = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")

        # Generate output
        with torch.no_grad():
            output = model.generate(
                input_ids=tokenized_input["input_ids"],
                attention_mask=tokenized_input["attention_mask"],
                max_new_tokens=OUTPUT_TOKENS
            )

        # Decode and print response
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        print("Response:", response)

model_names = ['mistralai/Mistral-7B-v0.1']
# model_names = ['mistralai/Mistral-7B-v0.1', 'EleutherAI/gpt-neo-2.7B']
# model_names = ['argilla/notus-7b-v1-lora']

for model_name in model_names:
    print(f"Processing model: {model_name}")

    # Load and optimize the model
    model, tokenizer = load_and_optimize_model(model_name)

    # Benchmark the model
    benchmark_model(model, tokenizer)

    print(f"Completed benchmarking for: {model_name}\n")

    # Running interactive inference
    run_inference(model, tokenizer)